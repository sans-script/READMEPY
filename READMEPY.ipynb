{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sans-script/READMEPY/blob/main/READMEPY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrX-7649MqGd"
      },
      "source": [
        "# Utilizando a API Google Generative AI na cria√ß√£o do chatbot ideal para a escrita de README's para o GitHub ü§ñ\n",
        "\n",
        "### Este Python Notebook implementa um chatbot utilizando a API Google Generative AI. Ap√≥s instalar e configurar as depend√™ncias, √© iniciado uma sess√£o de chat com o gemini-1.5-pro-latest. Para sair do chat, basta que o usu√°rio digite \"Exit\" (sem as aspas). As respostas do chatbot poder ser visualizadas com formata√ß√£o feita em Markdown, neste ambiente do Jupyter Notebook h√° um tratamento especial para mensagens que terminam com `--pure`. Essas respostas s√£o exibidas diretamente no console em Mark Down puro, sendo ideal para quem quer escrever um README.md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqHrKO-ZMqGh"
      },
      "source": [
        "A c√©lula abaixo executa dois comandos para instalar bibliotecas Python necess√°rias para o projeto:\n",
        "\n",
        "**1. `!pip install -q -U google-generativeai`**\n",
        "\n",
        "*   **`!pip`**: Indica que o comando deve ser executado usando o gerenciador de pacotes pip do Python.\n",
        "*   **`install`**: Instrui o pip a instalar um pacote.\n",
        "*   **`-q`**: Op√ß√£o para executar o comando em modo silencioso, sem exibir a sa√≠da detalhada.\n",
        "*   **`-U`**: Op√ß√£o para atualizar o pacote para a vers√£o mais recente, se j√° estiver instalado.\n",
        "*   **`google-generativeai`**: O nome do pacote a ser instalado. Este pacote fornece ferramentas e funcionalidades relacionadas a modelos de linguagem generativos do Google.\n",
        "\n",
        "**2. `!pip install -q -U tqdm`**\n",
        "\n",
        "*   **`!pip install -q -U`**: Mesma fun√ß√£o que no comando anterior, usando o pip para instalar ou atualizar um pacote em modo silencioso.\n",
        "*   **`tqdm`**: O nome do pacote a ser instalado. tqdm √© uma biblioteca popular para criar barras de progresso em Python, que podem ser √∫teis para visualizar o andamento de tarefas demoradas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR2KEM2WB7j-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai\n",
        "!pip install -q -U tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vSkO72uMqGl"
      },
      "source": [
        "### Aqui ser√° realizado os imports necess√°rios para a execu√ß√£o do projeto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqPrq78ICE45"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uzn-KY4MqGm"
      },
      "source": [
        "### Abaixo est√° sendo definido uma fun√ß√£o respons√°vel pela exibi√ß√£o da resposta em Mark Down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvYPKzHzMqGm"
      },
      "outputs": [],
      "source": [
        "def to_markdown(text):\n",
        "  text = text.replace('‚Ä¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHLWRp21MqGm"
      },
      "source": [
        "### Nesse trecho logo abaixo, est√° sendo configurado a API do Google para uso de seus modelos de AI generativa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxHrSFmnCUfI"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = userdata.get(\"SECRET_KEY\")\n",
        "genai.configure(api_key = GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-1.5-pro-latest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2srtbAV-MqGn"
      },
      "source": [
        "### Logo abaixo est√° o objeto `chat`, este objeto armazena entrada do usu√°rio combinada com a regra definida logo depois"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urdcEf5SC3dy"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat(history=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84PxduOrMqGo"
      },
      "source": [
        "### ‚ú® Descri√ß√£o do C√≥digo Python:\n",
        "\n",
        "Este trecho de c√≥digo implementa um loop de intera√ß√£o que processa a entrada do usu√°rio, gera uma resposta e a exibe formatada. A intera√ß√£o inclui personaliza√ß√£o com nome do usu√°rio.\n",
        "\n",
        "### Principais componentes:\n",
        "\n",
        "*   **`user_name`**: Armazena o nome do usu√°rio, obtido por meio de uma entrada.\n",
        "*   **`rule`**: Armazena uma string de instru√ß√£o que ser√° anexada √† entrada do usu√°rio antes de ser enviada para gera√ß√£o de resposta. A regra inclui o nome do usu√°rio.\n",
        "*   **`prompt`**: Vari√°vel que armazena a entrada do usu√°rio. Inicialmente, recebe uma mensagem de sauda√ß√£o personalizada com o nome do usu√°rio.\n",
        "\n",
        "**Loop Principal:**\n",
        "\n",
        "*   O loop `while` continua at√© que o usu√°rio digite \"Exit\".\n",
        "*   Dentro do loop:\n",
        "    *   **`response = chat.send_message(rule + prompt)`**: A entrada do usu√°rio √© combinada com a regra e enviada para um objeto `chat` atrav√©s do m√©todo `send_message`, para gerar uma resposta.\n",
        "     *   **Barra de progresso**: Uma barra de progresso √© exibida usando `tqdm` e `time.sleep` entre cada intera√ß√£o.\n",
        "     *   **Verifica√ß√£o de formata√ß√£o**: O c√≥digo verifica se a entrada do usu√°rio termina com \"--pure\".\n",
        "     *   Se terminar com \"--pure\":\n",
        "         *   A resposta √© exibida diretamente em Markdown puro.\n",
        "     *   Se n√£o terminar com \"--pure\":\n",
        "         *   A resposta √© convertida para markdown (visualiza√ß√£o da formata√ß√£o) usando `to_markdown` e exibida.\n",
        "     *   **Nova entrada**: Uma nova entrada √© solicitada ao usu√°rio.\n",
        "\n",
        "* Se ocorrerem erros, verifique se todas as depend√™ncias foram instaladas corretamente e execute novamente todas as c√©lulas em ordem de execu√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r1TAbY-E7Kl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "c165eeb8-a018-4335-899b-ea73210db850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qual o seu nome? Alex\n",
            "Ol√°, Alex. Posso ajudar? Que dia √© comemorado o dia da mentira e porqu√™?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading Response ‚ú®: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:01<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> O Dia da Mentira √© comemorado em **1¬∫ de abril**. A origem exata da data √© incerta, mas existem algumas teorias populares:\n> \n> * **Mudan√ßa do calend√°rio:** Uma teoria sugere que o Dia da Mentira surgiu na Fran√ßa no s√©culo XVI, quando o calend√°rio juliano foi substitu√≠do pelo calend√°rio gregoriano. No calend√°rio juliano, o ano novo era comemorado em 1¬∫ de abril, mas com a mudan√ßa, a data passou para 1¬∫ de janeiro. Aqueles que se esqueciam da mudan√ßa e continuavam a comemorar o ano novo em abril eram alvo de brincadeiras e eram chamados de \"bobos de abril\".\n> \n> * **Festivais pag√£os:** Outra teoria relaciona o Dia da Mentira a festivais pag√£os da primavera, como o festival romano de Hilaria, que envolvia disfarces e brincadeiras.\n> \n> Independentemente da origem, o Dia da Mentira se tornou uma tradi√ß√£o popular em muitos pa√≠ses, onde as pessoas pregam pe√ßas inofensivas umas nas outras nesse dia. \n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ Conte-me mais --pure\n",
            "\n",
            "\n",
            "A tradi√ß√£o do Dia da Mentira varia de pa√≠s para pa√≠s, mas algumas das brincadeiras mais comuns incluem:\n",
            "\n",
            "* **Not√≠cias falsas:** Jornais, revistas e sites de not√≠cias publicam hist√≥rias falsas para enganar os leitores.\n",
            "* **Pegadinhas:** As pessoas pregam pe√ßas umas nas outras, como colocar sal no a√ßucareiro ou mudar o hor√°rio do despertador.\n",
            "* **Brincadeiras na m√≠dia:** Programas de televis√£o e r√°dio podem transmitir not√≠cias falsas ou pregar pe√ßas em seus ouvintes.\n",
            "\n",
            "O Dia da Mentira √© uma oportunidade para se divertir e rir um pouco, mas √© importante lembrar de manter as brincadeiras leves e inofensivas. Ningu√©m quer ser a v√≠tima de uma piada de mau gosto. \n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_name = input(\"Qual o seu nome? \")\n",
        "rule = f'This is a rule: If you see the term --pure at the end of the prompt, i.e. the user message that is sent to you, do not mention it and do not talk about it in the response, just ignore it and continue reading the rest of the message and doing what is asked, and if there is no --pure in the user message, just ignore this message. If need the user name, the user name will be {user_name}'\n",
        "prompt = input(f\"Ol√°, {user_name}. Posso ajudar? \")\n",
        "\n",
        "for _ in tqdm(range(10), ncols=75, desc=\"Loading Response ‚ú®\"):\n",
        "    time.sleep(0.1)\n",
        "while prompt != \"Exit\":\n",
        "    response = chat.send_message(rule + prompt)\n",
        "    # response = model.generate_content(prompt) <- Descomente essa linha se vc quer que seu bot sofra de amn√©sia (n√£o guarde as mensagens no objeto chat)\n",
        "    md_rule = \"--pure\"\n",
        "    if prompt.endswith(md_rule):\n",
        "        print(response.text, \"\\n\")\n",
        "    else:\n",
        "        display(to_markdown(response.text))\n",
        "    prompt = input(\"$ \")\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}